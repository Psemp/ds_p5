{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.order_operations import get_min_max_dt, get_order_details  # Shows error but its ok, trust me\n",
    "from scripts.optimizers_mp import k_means_optimizer # Same\n",
    "from scripts.optimizers import dbscan_optimizer  # Same Same\n",
    "\n",
    "data_path = \"../data\"\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "load_dotenv()\n",
    "sns.color_palette('colorblind')\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "try:\n",
    "    pc_dpi = int(os.getenv('DPI'))\n",
    "except TypeError:\n",
    "    pc_dpi = 100\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Introduction\n",
    "\n",
    "# 2 : DataFrame creation, RFM logic\n",
    "\n",
    "# 3 : Early visualisations\n",
    "\n",
    "# 4 : Applying classification algorithms (K-Means, DBSCAN, Agglomerative clustering)\n",
    "\n",
    "# 5 : Conclusions and potential improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>1 : Introduction :</u>\n",
    "\n",
    "&emsp;The attempt of this notebook is focused on the widespread approach : RFM (Recency Frequency Monetary). Usual classifications using traditionnal (and fundamentaly flawed) approaches will likely be atempted, like the Paretto distribution.<br>\n",
    "\n",
    "&emsp;Using RMF approach, we will first attempt to distinguish classes with the data AS IS - This is expected to produce poor results. We will then evolve towards classifying machine learning algorithms which use more resources but produce, in general, better results.<br>\n",
    "\n",
    "&emsp;It is expected that the data provided by this approach will be insufficient but will provide a good raw material to improve the models by improving the data quality and the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>2 : DataFrame creation, RFM logic</u>\n",
    "\n",
    "&emsp;The usage of RFM presupposes that we have the data to calculate the 3 indicators (R F & M). Not all datasets will be required for this step and there is no need to clog up the memory as the calculation time/resource is expected to go up.<br>\n",
    "&emsp;Will be required : <br>\n",
    "- olist_customers : will provide the link between the unique customer and it's aliases (and orders) - The goal is to eliminate the useless repetitivity of the alias.\n",
    "- olist_orders : will provide the link between the customer (referred sometimes as cx.) and the order itself - The date the order was placed and which alias placed the order.\n",
    "- olist_order_items : will provide what items were ordered at what price, determining the monetary component of each order\n",
    "\n",
    "<hr>\n",
    "\n",
    "<i>\n",
    "&emsp;The Recency will be defined by the time separating the most recent update from a Cx and the most recent update known - In this approach, we assume that the latest order placed is the most recent order (instead of using t0 = today).<br>\n",
    "&emsp;The Frequency will be : number_of_orders/membership_time. membership_time is : time elapsed between first order of account and last general order(not last order of account).<br>\n",
    "&emsp;Monetary will be the total of all item prices of all orders placed by a customer, Kaggle shows that freight is always paid (case where Cx orders 10 times the same item from the same seller --> Cx will pay 10 times the freight price).\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olist_customers_file = \"../data/optimized/olist_customers.csv\"\n",
    "olist_orders_file = \"../data/optimized/olist_orders.csv\"\n",
    "olist_order_items_file = \"../data/optimized/olist_order_items.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = pd.read_csv(filepath_or_buffer=olist_customers_file)\n",
    "df_orders = pd.read_csv(filepath_or_buffer=olist_orders_file)\n",
    "df_orders_items = pd.read_csv(filepath_or_buffer=olist_order_items_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dtypes were not carried over and will need to be enforced\n",
    "# Or using pickles rather than CSV\n",
    "\n",
    "df_customers[\"customer_id\"] = df_customers[\"customer_id\"].astype(np.uint32)\n",
    "df_customers[\"customer_unique_id\"] = df_customers[\"customer_unique_id\"].astype(np.uint32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\n",
    "        \"order_purchase_dt\", \"order_approved_at\",\n",
    "        \"order_delivered_carrier_date\", \"order_delivered_customer_date\",\n",
    "        \"order_estimated_delivery_date\"\n",
    "    ]\n",
    "\n",
    "for col in date_cols:\n",
    "    df_orders[col] = pd.to_datetime(df_orders[col])\n",
    "\n",
    "df_orders[\"order_id\"] = df_orders[\"order_id\"].astype(np.uint32)\n",
    "df_orders[\"customer_id\"] = df_orders[\"customer_id\"].astype(np.uint32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_items[\"order_id\"] = df_orders_items[\"order_id\"].astype(np.uint32)\n",
    "df_orders_items[\"order_item_id\"] = df_orders_items[\"order_item_id\"].astype(np.uint32)\n",
    "df_orders_items[\"product_id\"] = df_orders_items[\"product_id\"].astype(np.uint32)\n",
    "df_orders_items[\"seller_id\"] = df_orders_items[\"seller_id\"].astype(np.uint32)\n",
    "\n",
    "df_orders_items[\"shipping_limit_date\"] = pd.to_datetime(df_orders_items[\"shipping_limit_date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_items.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_items.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point can be orders df, we will remove infos as we need\n",
    "\n",
    "df_orders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cx id, order_id, order_purchase_dt are useful, the rest can go\n",
    "\n",
    "df_orders = df_orders[[\"order_id\", \"customer_id\", \"order_purchase_dt\"]]\n",
    "\n",
    "df_orders[\"customer_uid\"] = np.uint32(0)\n",
    "df_orders[\"sum_total\"] = np.nan\n",
    "\n",
    "df_orders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_ids = df_orders[\"order_id\"].unique()\n",
    "\n",
    "for order_id in order_ids:\n",
    "    index = df_orders.index[df_orders[\"order_id\"] == order_id][0]\n",
    "    order_value = df_orders_items[df_orders_items[\"order_id\"] == order_id][\"price\"].sum()\n",
    "    freight_value = df_orders_items[df_orders_items[\"order_id\"] == order_id][\"freight_value\"].sum()\n",
    "    df_orders.at[index, \"sum_total\"] = order_value + freight_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting customers unique ids instead of cx_ids\n",
    "\n",
    "cx_ids = df_orders[\"customer_id\"]\n",
    "\n",
    "for cx_id in cx_ids:\n",
    "    index = df_orders.index[df_orders[\"customer_id\"] == cx_id][0]\n",
    "    cx_uid = df_customers[df_customers[\"customer_id\"] == cx_id][\"customer_unique_id\"]\n",
    "    df_orders.at[index, \"customer_uid\"] = cx_uid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_cols = [\"customer_uid\", \"order_id_list\", \"most_ancient_order_dt\", \"most_recent_order_dt\", \"recency\", \"frequency\", \"monetary\"]\n",
    "df_rfm = pd.DataFrame(columns=rfm_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = df_orders[\"customer_uid\"].unique()\n",
    "\n",
    "uniques.sort()\n",
    "\n",
    "df_rfm[\"customer_uid\"] = uniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uniques:\n",
    "    index = df_rfm.index[df_rfm[\"customer_uid\"] == uid][0]\n",
    "    details = get_order_details(cx_uid=uid, uid_col=\"customer_uid\", from_frame=df_orders)\n",
    "    df_rfm.at[index, \"order_id_list\"] = details[\"order_list\"]\n",
    "    df_rfm.at[index, \"monetary\"] = details[\"total_spent\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tuple in df_rfm.itertuples():\n",
    "    index = tuple.Index\n",
    "    order_list = tuple.order_id_list\n",
    "    min_max_dt = get_min_max_dt(order_list=order_list, from_frame=df_orders, dt_col=\"order_purchase_dt\")\n",
    "    df_rfm.at[index, \"most_ancient_order_dt\"] = min_max_dt[\"min\"]\n",
    "    df_rfm.at[index, \"most_recent_order_dt\"] = min_max_dt[\"max\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_global = df_rfm[\"most_recent_order_dt\"].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recency(row, most_recent_global) -> pd.Timedelta:\n",
    "    \"\"\"\n",
    "    Returns timedelta in seconds between most recent purchase global and most recent purchase cx\n",
    "    \"\"\"\n",
    "    return (most_recent_global - row[\"most_recent_order_dt\"]).total_seconds()\n",
    "\n",
    "\n",
    "def get_frequency(row, most_recent_global):\n",
    "    \"\"\"\n",
    "    Returns avg. purchases made per active month\n",
    "    \"\"\"\n",
    "    account_timespan = most_recent_global - row[\"most_ancient_order_dt\"]\n",
    "    try:\n",
    "        return len(row[\"order_id_list\"]) / (account_timespan.total_seconds() / 2419200)  # Purchase per active month\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_num_order(row):\n",
    "    return len(row[\"order_id_list\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm[\"recency\"] = df_rfm.apply(get_recency, axis=1, args=(most_recent_global, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm[\"frequency\"] = df_rfm.apply(get_frequency, axis=1, args=(most_recent_global, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm[\"num_orders\"] = df_rfm.apply(get_num_order, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - conclusion :\n",
    "\n",
    "- Many cells are not useful and can be safely deleted (all cells displaying dfs and head() / info())\n",
    "- DataFrame created containing RFM values : \n",
    "    - Recency is delta T in seconds between Cx most recent order and Global most recent order\n",
    "    - Frequency is purchases made / account creation to most recent order | In purchase per month\n",
    "    - Monetary is total spent on all order, including freight which is -always- paid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>3 : Early visualisations</u>\n",
    "\n",
    "Goals : \n",
    "- Identify possible clusters using only RFM and scaling (most likely min-max scaler)\n",
    "- Apply Pareto Principle (bs) : 20% of customers generate 80% of traffic\n",
    "- Identify possibly lacking variables overlooked by RFM method (to be confirmed by part 4)\n",
    "- Plot RFM two by two, then 3D attempt and/or radar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> 1 : Intuition : Most clients make just one purchase</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.sort_values(\"num_orders\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_orders = df_rfm[\"num_orders\"].value_counts().to_dict()\n",
    "\n",
    "number_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(12, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ax1.bar(x=list(number_orders.keys()), height=(number_orders.values()), width=1, color=\"navy\", edgecolor=\"black\")\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Number of orders\")\n",
    "ax1.set_xticks(range(0, max(list(number_orders.keys()))))\n",
    "ax1.set_ylabel(\"Number of customers\")\n",
    "fig.suptitle(\"Number of orders distribution in dataset\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding 0 : \n",
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(12, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "try:\n",
    "    number_orders.pop(0)\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "ax1.bar(x=list(number_orders.keys()), height=number_orders.values(), width=1, color=\"navy\", edgecolor=\"black\")\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Number of orders\")\n",
    "ax1.set_ylabel(\"Number of customers\")\n",
    "ax1.set_xticks(range(1, max(list(number_orders.keys()))))\n",
    "fig.suptitle(\"Number of orders per cx, exclusion of unique orders\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations : \n",
    "\n",
    "&emsp;Clearly, olist has either problems keeping customers \"loyals\" or it is easier to create a new account each visit. In any case, the amount of customers who made exactly one purchase is crushingly larger than all other customers combined, regardless of frequency (93099 customers).\n",
    "&emsp;Excluding customers who made exactly one purchase, the majority of remaining customers order on olist between .1 and .4 times a month (between 1.1 and 4.8 times a year). There are extremes ordering up to 1.7 times a month. But these customers are extremely rare.\n",
    "\n",
    "- Operating under the assumption that Olist did not provide its whole database dump, early conclusions cannot be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>2 : How much have customers spent ?</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(12, 5),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "flierprops={\"marker\": \"+\", \"markersize\": 2, \"markerfacecolor\": \"navy\"}\n",
    "\n",
    "ax1.boxplot(x=\"monetary\", data=df_rfm, showmeans=True, vert=False, flierprops=flierprops)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "fig.suptitle(\"Customer repartition by total spent on Olist\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm[\"monetary\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of zeroes, either purchased is cancelled or refunded. - Lets drop those and zoom on 0 -> 1K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(12, 5),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "flierprops={\"marker\": \"+\", \"markersize\": 2, \"markerfacecolor\": \"navy\"}\n",
    "\n",
    "ax1.boxplot(x=\"monetary\", data=df_rfm[df_rfm[\"monetary\"] != 0.0], showmeans=True, vert=False, flierprops=flierprops)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlim(0, 1000)\n",
    "fig.suptitle(\"Customer repartition by total spent on Olist | Transaction = 0 removed & Zoom on 0-1K\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason .describe() does not work here\n",
    "print(\"Q1 :\", df_rfm[\"monetary\"].quantile(.25))\n",
    "print(\"Q3 :\", df_rfm[\"monetary\"].quantile(.75))\n",
    "print(\"Median :\", df_rfm[\"monetary\"].median())\n",
    "print(\"Avg. :\", np.average(df_rfm[\"monetary\"].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df_rfm[df_rfm[\"monetary\"] < 62.39]\n",
    "outliers = df_rfm[df_rfm[\"monetary\"] > 182.2375]\n",
    "print(\"Max :\", max(df_rfm[\"monetary\"].values))\n",
    "print(len(outliers))\n",
    "del outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations :\n",
    "\n",
    "&emsp;Most clients spend between 62.39 & 182.24 R$ (Assuming it's in Reales and not in Dollars, it's not precised in the dataset and website screenshots show that R$ is used), with the median at 107.28 and an average of 164.88 R$.<br>\n",
    "&emsp;There is a wide range of outliers (24024) with spendings going from +Q3(182.2375 R$) to 13664.08 R$, but, mostly, outliers spent 4K max with a very small number of customers crossing this line.\n",
    "\n",
    "- Operating under the assumption that Olist did not provide its whole database dump, early conclusions cannot be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> 3 : Time Delta between global last order and customer last order</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.describe()[\"recency\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(12, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.boxplot(x=\"recency\", data=df_rfm, vert=False, showmeans=True)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "fig.suptitle(\"Distribution of customers based on their last order time:\")\n",
    "ax1.set_xlabel(\"Time Delta (seconds)\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not very visual so let's convert seconds to days : \n",
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(12, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "deltas = df_rfm[\"recency\"]\n",
    "deltas_days = np.divide(deltas, 86400)\n",
    "\n",
    "ax1.boxplot(x=deltas_days, vert=False, showmeans=True)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "fig.suptitle(\"Distribution of customers based on their last order time :\")\n",
    "ax1.set_xlabel(\"Time Delta (days)\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_days.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data that most customers haven't ordered in a while\n",
    "\n",
    "- 75% of the customers in the database haven't ordered anything in at least the last 164 days (rounded up).\n",
    "- Operating under the assumption that Olist did not provide its whole database dump, early conclusions cannot be drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.pairplot((df_rfm[[\"recency\", \"frequency\", \"monetary\"]]))\n",
    "\n",
    "grid.figure.figsize = (4, 4)\n",
    "grid.figure.dpi = pc_dpi\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "grid.figure.suptitle(\"Pairplot between RFM variables\")\n",
    "#\n",
    "###\n",
    "grid.figure.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "&emsp;There are no obvious clusters distinguishable using simply RFM variables and pairploting. 3D Plotting of RFM can be attempted but it seems the data of the 3 variables will not be sufficient to offer what the customer (Olist) is looking for.\n",
    "&emsp;The Wikipedia page regarding Olist mentions that there are 2M+ unique active customers, so it looks like we only have a small fragment of their database : we know we have 96K accounts and it exists 2M+ accounts, but we are not aware of other useful stastistics, like the number of order, which would explain why our frequency is = 0 in close to 99% of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 : 3D Visualisation of RFM variables\n",
    "\n",
    "We hope to obtain visual clusters to identify and classify customers using the MK1 eyeball without any clustering algorithms or data engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict ={\n",
    "        \"recency\": \"Recency (in seconds)\",\n",
    "        \"frequency\": \"Frequncy of purchase(s)\",\n",
    "        \"monetary\": \"Total Spent on Olist\"\n",
    "    }\n",
    "\n",
    "marker_style = {\n",
    "    \"color\": 'navy',\n",
    "    \"size\": 5,\n",
    "    }\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    data_frame=df_rfm, x=\"recency\",\n",
    "    y=\"frequency\", z=\"monetary\",\n",
    "    width=5 * pc_dpi, height=3 * pc_dpi,\n",
    "    labels=labels_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=40, r=40, t=40, b=40),\n",
    "    title=\"3D Representation of customers, RFM approach\",\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=marker_style)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclear\n",
    "\n",
    "- The most recent \"order\" is not really an order as it sum total is 0. Maybe it was too recent for the payment for being processed, awaiting further directions for actions on total_spent = 0\n",
    "- We can clearly see the extremes and what looks like a zone where most Cxs are placed, but the clustering is still unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>4 : Applying classification algorithms (K-Means, DBSCAN, Agglomerative clustering)</u>\n",
    "\n",
    "> Following the inconclusive results (as expected) of part #3, Machine Learning classification algorithms can be applied to determine if clusters can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 : Min Max Scaling the dataset\n",
    "\n",
    "> 0 -> 10 or 0 -> 5 will be better suited for understanding data like reviews and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler(feature_range=(0, 10))\n",
    "\n",
    "dropcols = [\"order_id_list\", \"most_ancient_order_dt\", \"most_recent_order_dt\", \"num_orders\"]\n",
    "\n",
    "df_rfm_mms = df_rfm.drop(columns=dropcols)\n",
    "\n",
    "df_rfm_mms.set_index(\"customer_uid\", inplace=True)\n",
    "\n",
    "keepcols = df_rfm_mms.columns\n",
    "\n",
    "df_rfm_mms = mms.fit_transform(df_rfm_mms.to_numpy())\n",
    "\n",
    "df_rfm_mms = pd.DataFrame(df_rfm_mms, columns=keepcols)\n",
    "\n",
    "df_rfm_mms.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 : Using and optimizing k-means clustering\n",
    "\n",
    "> Trial and error will be used to determine the best k-range to pass to the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2, 17)\n",
    "\n",
    "k_means_optimizer(data=df_rfm_mms, k_range=k_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations :\n",
    "\n",
    "&emsp; 4 and 5 both seem to be the optimal k-numbers of clusters, the error keeps diminishing with the increasing number of clusters but the silhouette score is acceptable, performing k-means with k=4 (k=5 doesnt look as good) and plotting the results\n",
    "<br>\n",
    "><i>It seems the silhouette score calculation is what makes the optimizer takes it's sweet sweet time, if not needed for comparison, it can be considered to rely solely on intertia</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4)\n",
    "y_predicted = km.fit_predict(df_rfm_mms)\n",
    "\n",
    "df_rfm_mms[\"cluster_4\"] = y_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict ={\n",
    "        \"recency\": \"Recency\",\n",
    "        \"frequency\": \"Frequency of purchase(s)\",\n",
    "        \"monetary\": \"Total Spent on Olist\"\n",
    "    }\n",
    "\n",
    "marker_style = {\n",
    "    \"size\": 5,\n",
    "    }\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    data_frame=df_rfm_mms, x=\"recency\",\n",
    "    y=\"frequency\", z=\"monetary\", color=\"cluster_4\",\n",
    "    width=4 * pc_dpi, height=3 * pc_dpi,\n",
    "    labels=labels_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=40, r=40, t=40, b=40),\n",
    "    title=\"3D Representation of customers, RFM approach, k-means clustering, k=4\",\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=marker_style)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :\n",
    "\n",
    "&emsp;Even if the clustering is not crystal clear, we can see a somewhat clear pattern, to be expected. :\n",
    "\n",
    "- Cluster 4 (Farthest cluster to origin, cluster# changes) represents disatisfied people overall, this group's mind might not be possible to change about the goals and ideas of Olist : we can theorize (and calculate later) that this group purchased mainly once, for a moderate sum of Reales, and a long time ago. \n",
    "- Cluster 2 and 3 (Center groups, cluster# changes) represents people who like Olist, they might not be huge enthusiasts pressing refresh every 10seconds each time Olist releases a new feature but they might be the most important customers, those on which Olist might need to focus the most. Frequency is not really reliable with the data at hand but this group shows a net increase in this statistic, meanwhile their purchases have been more recent and mostly more profitable than Cluster 4\n",
    "- Cluster 1 (closest cluster to origin, cluster# changes) represents the people who believe strongly in the services Olist provides, it is very clear that this group contains the most repeating customers, who consequently put on their last order the most recently and who are ready to pay a higher price for Olist's services.\n",
    "\n",
    "&emsp;Even if it is too early to tell, we can speculate and maybe try to apply the \"Law of Diffusion of Innovation\", Everett Rogers' 1968 theory and reapplied to business models by the now renowned business writer and public speaker Simon Sinek. <br><br>\n",
    "&emsp;This theory states that :\n",
    "- The first 2.5% of Investors are the innovators, believing strongly in the compagny's ideas and willing to take risks on said idea\n",
    "- The next 13.5% are the Early adopters, willing to take risks to try a new model/piece of technology etc.\n",
    "- The next 34% are the Early Majority and the following 34% are the Late Majority, investing or using the product/service because it is trending and tested by the first two groups\n",
    "- The last 16% are the laggards, who wont willingly make any effort or take any risk for an innovation, newer product or service, this group is not swayed by advertising, word of mouth etc. and represent poor investment in marketing\n",
    "&emsp;This Law also theorizes that the tipping point for a new product/service to really take off, it needs the first 15% of these people, group 1 and 2, and that the rest of the majority, where the majority of consumers are, will follow\n",
    "\n",
    "&emsp;We can quantify our clusters and see if we can indeed see what looks like this distribution : group 1 & 2 believing strongly in the idea (cluster 1), group 3 & 4 being the majority (cluster 0 and 2) and group 5 (cluster 3) being the laggards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = int(input(\"Investors : \"))\n",
    "maj1 = int(input(\"Early Majority\"))\n",
    "maj2 = int(input(\"Late Majority\"))\n",
    "lagg = int(input(\"Laggards\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(12, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "amounts = df_rfm_mms[\"cluster_4\"].value_counts()\n",
    "amounts_inv = amounts[inv]\n",
    "amount_maj_1 = amounts[maj1]\n",
    "amount_maj_2 = amounts[maj2]\n",
    "amounts_maj = amount_maj_1 + amount_maj_2\n",
    "amount_lag = amounts[lagg]\n",
    "total = len(df_rfm_mms)\n",
    "\n",
    "cluster_dict = {\n",
    "        f\"Investors-{round((amounts_inv / total) * 100, ndigits=2)}%\": amounts_inv,\n",
    "        f\"Majority-{round((amounts_maj / total) * 100, ndigits=2)}%\": amounts_maj,\n",
    "        f\"Laggards-{round((amount_lag / total) * 100, ndigits=2)}%\": amount_lag\n",
    "    }\n",
    "\n",
    "my_colors = [\"royalblue\", \"#003153\", \"red\"]\n",
    "\n",
    "ax1.bar(\n",
    "        x=list(cluster_dict.keys()),\n",
    "        height=list(cluster_dict.values()),\n",
    "        color=my_colors\n",
    "    )\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Law of Diffusion Classes (with respective percentages)\")\n",
    "ax1.set_ylabel(\"Customer per Classes\")\n",
    "fig.suptitle(\"Expression of Customers theorizing the Law of Diffusion of Innovation\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :\n",
    "\n",
    "&emsp;The above graph seems to confirm the intuition that Olist's customers follow indeed the Law of Diffusion of Innovation (or cunsumption here). It is very interesting to see that the cuts between the classes are clear and provide a potential course of action for Olist.\n",
    "&emsp;Indeed, according to the Law of Diffusion of Innovation : \n",
    "- Our \"Investors\" (here around 20% based on each run) are already satisfied with Olist, its services and its methods. They are happy with Olist as is and necessitate little to no effort to keep loyal.\n",
    "- On the other hand, the \"Laggards\" (here around 18%) could be interpreted as customers who are either not interested in the services Olist provides, do not wish to change their habbits of consumption or are generally disappointed by the services ; this group -cannot- be swayed, or at a high cost.\n",
    "- The Majority (Early and Late on the above graph are merged) are the group on which Olist should concentrate its marketting efforts : they are often the followers of the first group. On a purely economical standpoint, this is the most important group to satisfy while keeping group 1 happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 : DBSCAN Clustering :\n",
    "\n",
    "&emsp;DBSCAN is another algorithm testable on this dataset. We first need to determine the best max intra cluster distance (epsilon) and the best minimal amount of points it takes to make a cluster, this is often dimension + 1 or dimension * 2\n",
    "&emsp;Epsilon can be determined using kneighbors. Using a graph to represent the avg distance between a point and its k-neighbors (here 4 : dimension + 1). Zooming in and using the elbow method help us to focus on the best potential epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix = df_rfm_mms[[\"recency\", \"frequency\", \"monetary\"]].to_numpy()\n",
    "nneighbors = NearestNeighbors(n_neighbors=4, n_jobs=-1)  # dataset dim + 1\n",
    "\n",
    "nneighbors.fit(X=neighbors_matrix)\n",
    "\n",
    "distances, potential_eps = nneighbors.kneighbors(neighbors_matrix)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances_plot = distances[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(16, 8),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.plot(distances_plot)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Object\")\n",
    "ax1.set_ylabel(\"k distance\")\n",
    "fig.suptitle(\"Points sorted by distance - Neighbors = 4\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming up until we can see the \"eblow\"\n",
    "\n",
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(16, 8),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.plot(distances_plot)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Object\")\n",
    "ax1.set_ylabel(\"k distance\")\n",
    "ax1.set_xlim((96020, 96100))\n",
    "ax1.set_yticks(np.arange(0, 5, 0.1))\n",
    "ax1.grid(visible=True, axis=\"both\")\n",
    "fig.suptitle(\"Points sorted by distance - Neighbors = 4\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like .8 is the best candidate\n",
    "\n",
    "best_dbs = DBSCAN(eps=0.8, min_samples=4, n_jobs=-1)\n",
    "\n",
    "y_predict = best_dbs.fit_predict(\n",
    "        df_rfm_mms.drop(\n",
    "                columns=[\"cluster\", \"cluster_4\", \"cluster_5\", \"cluster_DBSCAN\"], errors=\"ignore\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    df_rfm_mms.loc[:, \"cluster_DBSCAN\"] = y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict ={\n",
    "        \"recency\": \"Recency\",\n",
    "        \"frequency\": \"Frequency of purchase(s)\",\n",
    "        \"monetary\": \"Total Spent on Olist\"\n",
    "    }\n",
    "\n",
    "marker_style = {\n",
    "    \"size\": 5,\n",
    "    }\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    data_frame=df_rfm_mms, x=\"recency\",\n",
    "    y=\"frequency\", z=\"monetary\", color=\"cluster_DBSCAN\",\n",
    "    width=4 * pc_dpi, height=3 * pc_dpi,\n",
    "    labels=labels_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=40, r=40, t=40, b=40),\n",
    "    title=\"3D Representation of customers, RFM approach, DBSCAN clustering\",\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=marker_style)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :\n",
    "\n",
    "&emsp;DBSCAN clustering is disappointing here. It identifies outliers rather than smaller groups. It doesnt help to narrow down the potential targets for any campaign. It is unclear whether or not my parameters are not correct or if the algorithm is not helpful in this case.\n",
    "&emsp;A final clustering attempt can be made using Agglomerative Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 : Agglomerative Clustering\n",
    "\n",
    "&emsp;Agglomerative Clustering might help identify pre-determined clusters with the help of the linkage distance, which we can use from scipy\n",
    "&emsp;The objective is to produce at least a similar result to K-Means, if not, it is not worth using and updating at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 : Determining the correct amount of clusters :\n",
    "\n",
    "&emsp;Quoting sources : `If you want to create flat clusters we can analyze the [...] dendrogram to determine no. of clusters. We first assume that the horizontal lines are extended on both sides, and as such, they would also cross the vertical lines. Now we have to identify the tallest vertical line that does not have any horizontal line crossing through it.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Due to Apple m1 chip being a little stupid, we need to sample the group or the overpriced piece of aluminium is gonna melt\n",
    "## Taking representatives from classes from k-means, sample size = 2K\n",
    "sample_inv = df_rfm_mms[df_rfm_mms[\"cluster_4\"] == inv].sample(round((amounts_inv / len(df_rfm_mms)) * 2000))\n",
    "sample_maj1 = df_rfm_mms[df_rfm_mms[\"cluster_4\"] == maj1].sample(round((amount_maj_1 / len(df_rfm_mms)) * 2000))\n",
    "sample_maj2 = df_rfm_mms[df_rfm_mms[\"cluster_4\"] == maj2].sample(round((amount_maj_2 / len(df_rfm_mms)) * 2000))\n",
    "sample_lagg = df_rfm_mms[df_rfm_mms[\"cluster_4\"] == lagg].sample(round((amount_lag / len(df_rfm_mms)) * 2000))\n",
    "\n",
    "df_rfm_mms_2ksample = pd.concat([sample_inv, sample_maj1, sample_maj2, sample_lagg]).sort_index()\n",
    "\n",
    "len(df_rfm_mms_2ksample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 4),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "linkage_method = hierarchy.linkage(\n",
    "        df_rfm_mms_2ksample.drop(columns=[\"cluster\", \"cluster_4\", \"cluster_5\", \"cluster_DBSCAN\"], errors=\"ignore\"),\n",
    "        method =\"ward\",\n",
    "        metric=\"euclidean\",\n",
    "    )\n",
    "dendrogram_plot = hierarchy.dendrogram(linkage_method, ax=ax1, no_labels=True)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say its 3 ?\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "#Let's try the small one first\n",
    "df_rfm_mms_2ksample[\"cluster_agg\"] = agg.fit_predict(df_rfm_mms_2ksample.drop(columns=[\"cluster_4\", \"cluster_DBSCAN\"], errors=\"ignore\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_mms_2ksample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict ={\n",
    "        \"recency\": \"Recency\",\n",
    "        \"frequency\": \"Frequency of purchase(s)\",\n",
    "        \"monetary\": \"Total Spent on Olist\"\n",
    "    }\n",
    "\n",
    "marker_style = {\n",
    "    \"size\": 5,\n",
    "    }\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    data_frame=df_rfm_mms_2ksample, x=\"recency\",\n",
    "    y=\"frequency\", z=\"monetary\", color=\"cluster_agg\",\n",
    "    width=4 * pc_dpi, height=3 * pc_dpi,\n",
    "    labels=labels_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=40, r=40, t=40, b=40),\n",
    "    title=\"3D Representation of customers, RFM approach, Agglomerative clustering - 2K sample\",\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=marker_style)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation :\n",
    "This clustering method could be fine, and sure, it works on a 2K sample but an I7 4.3GhZ w/ 32Gb RAM couldn't hack it in an hour for 100K. So it is not in the interest of Olist to use an algorithm that demanding for their 2M unique customers.\n",
    "\n",
    "Test has been done on Desktop using :\n",
    "- i7-97000K\n",
    "- 32Gb RAM 2133 MhZ\n",
    "- NVidia 3080 4GB integrated RAMDAC\n",
    "- Windows 10\n",
    "- Visual Studio Code with Python 3.10.5\n",
    "<br><hr>\n",
    "Test was stopped at the one hour mark on the 96 000 cx sample because it was simply not feasible, much less scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>5 : Conclusion and potential improvements :</u>\n",
    "\n",
    "- K-Means clustering shows a real way of clustering the customers, the theory is backed up by a proven by time \"law\" : the diffusion of innovation, this is only a primary result but it narrows down considerably Olist's targets.\n",
    "- Further improvements could be, and will be made, using and testing metrics like :\n",
    "    - Geolocation : how the customers are clustered and where a campaign can have the maximum impact\n",
    "    - Delta Dist between buyers and sellers : Do customers consume close to home ? Does this matter ?\n",
    "    - Reviews : Are consummers driven to a product because of it's previous ratings, and do potential loyal customer leave reviews more often ?\n",
    "    - Categories : Are some categories more successful than others ? Are giants like Amazon and Ebay swallowing a given category while leaving others to local businesses, favored by Olist ?\n",
    "\n",
    "Not all of this points might be relevant but including them in the model could make a difference.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### <b>Export :</b>\n",
    "<u>Will be exported :</u>\n",
    "- customers unique ids\n",
    "- the list of their order(s)\n",
    "- the RFM data\n",
    "- the initial clusters found by k-means --> Unexpectedly positive results could mean improvements on further models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2214c6fb926f71468a18ca7cf946224dc61b1c77c0f95ec2e3a3e68abe6a5ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
