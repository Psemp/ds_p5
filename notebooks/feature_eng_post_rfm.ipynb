{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from scripts.distances import haversine_km  # shows an error but its a false positive (we added ../ to path)\n",
    "\n",
    "data_path = \"../data\"\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "load_dotenv()\n",
    "sns.color_palette('colorblind')\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "try:\n",
    "    pc_dpi = int(os.getenv('DPI'))\n",
    "except TypeError:\n",
    "    pc_dpi = 100\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_data = \"../data/optimized/olist_orders.csv\"\n",
    "rfm_dataset = \"../pickles/geolocd/cx_rfm.plk\"\n",
    "customer_dataset = \"../data/optimized/geoloc_applied/olist_customers.csv\"\n",
    "sellers_dataset = \"../data/optimized/geoloc_applied/olist_sellers.csv\"\n",
    "reviews_data = \"../data/optimized/olist_reviews.csv\"\n",
    "order_items_data = \"../data/optimized/olist_order_items.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = pd.read_csv(order_data)\n",
    "df_rfm = pd.read_pickle(rfm_dataset)\n",
    "df_cx = pd.read_csv(customer_dataset)\n",
    "df_sellers = pd.read_csv(sellers_dataset)\n",
    "df_items = pd.read_csv(order_items_data)\n",
    "df_reviews = pd.read_csv(reviews_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 : Getting the variables we need from other datasets\n",
    "# 1 : Calculate Time deltas for orders\n",
    "# 2 : Caculate Dist delta of seller - buyers\n",
    "# 3 : Transitionning from the orders dataset to the cx_rfm dataset.\n",
    "\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> 0 : Getting the variables we need from other datasets :</u>\n",
    "\n",
    "- ## Goals\n",
    "    - Joining the datasets of order and order_items on order_id to know the distance between the buyer and seller\n",
    "    - Joining the datasets of order and reviews to know the reviews for reviewed orders.\n",
    "\n",
    "<br>\n",
    "\n",
    "- ## Whys :\n",
    "    - Export for further feature engineering\n",
    "    - Make the datasets less codependents, centralize infos in as few files as possible\n",
    "    - We will work with only two datasets (orders and cx_rfm)\n",
    "\n",
    "- ## Hows : \n",
    "    - >we'll see\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> 0.1 : Products and Items for each orders :</u>\n",
    "\n",
    "### How :\n",
    "\n",
    "We will use order_id to find items and item sellers in olist_order_items dataset as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if df_items contains duplicated order_id, if not, using order id as index\n",
    "df_items[\"order_id\"].duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">That was a long shot anyway, but we cannot use order_id as index for df_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a selection on items : \n",
    "# Already in RFM, shipping date is not relevant\n",
    "\n",
    "df_items = df_items.drop(columns=[\"shipping_limit_date\", \"price\", \"freight_value\", \"order_item_id\"])\n",
    "\n",
    "df_items.sort_values(by=\"order_id\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the product_list and seller_list for each order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"product_list\"] = np.dtype(\"object\")\n",
    "df_order[\"seller_list\"] = np.dtype(\"object\")\n",
    "\n",
    "order_ids = df_order[\"order_id\"].values\n",
    "\n",
    "for current_order in order_ids:\n",
    "    order_idx = df_order[df_order[\"order_id\"] == current_order].index[0]\n",
    "    items = df_items[df_items[\"order_id\"] == current_order]\n",
    "    df_order.at[order_idx, \"product_list\"] = list(items[\"product_id\"].values)\n",
    "    df_order.at[order_idx, \"seller_list\"] = list(items[\"seller_id\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looks good, now, we know that an order can contain multiple products, but can an order mobilize multiple sellers ? Let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_sellers = 0\n",
    "\n",
    "for tuple in df_order.itertuples():\n",
    "    if len(tuple.seller_list) > 1:\n",
    "        multiple_sellers += 1\n",
    "\n",
    "print(f\"there are {multiple_sellers} orders with multiple sellers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So yes, an order can totally mobilize multiple sellers, hence preventing us from changing the type of \"seller_list\" from list (or object in a dataframe) to a unique int. That also raises questions \n",
    "\n",
    "- Do we keep these multiple sellers and compute the average distance between the customer and these multiple sellers ?\n",
    "- Do we take the avg. position of a unique \"Seller_geoloc\" using the mean position of all the multiple sellers ?\n",
    "- Do we select a particular seller based on an agreed upon metric of characteristic ?\n",
    "\n",
    "For now : we will take ∆avg for each order.\n",
    "<br>\n",
    "<i>In it's own section</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> 0.2 : Geolocs from sellers and buyers for each order :</u>\n",
    "\n",
    "- We will use the impractical tuple (lat/lon) and unpack it when we will have to plot on a map.\n",
    "- Buyer will have only one geoloc, seller_list will be a list of lat/lon\n",
    "\n",
    "> First let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sellers.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How : \n",
    "Using the same method as in the order/items, we will find the lat/lon tuple for each seller and buyer using : customer_id and seller_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"geoloc_cx\"] = np.dtype(\"object\")\n",
    "\n",
    "cx_ids = df_order[\"customer_id\"].values\n",
    "\n",
    "for current_order in order_ids:\n",
    "    order_idx = df_order[df_order[\"order_id\"] == current_order].index[0]\n",
    "    cx_id = df_order[df_order[\"order_id\"] == current_order][\"customer_id\"].values[0]\n",
    "    cx_row = df_cx[df_cx[\"customer_id\"] == cx_id]\n",
    "    lat, lon = float(cx_row[\"lat\"].values[0]) , float(cx_row[\"lon\"].values[0])\n",
    "    lat_lon = (lat, lon)\n",
    "    df_order.at[order_idx, \"geoloc_cx\"] = lat_lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"geoloc_sellers_list\"] = np.dtype(\"object\")\n",
    "\n",
    "sellers_ids = df_order[df_order[\"order_id\"] == current_order].index[0]\n",
    "\n",
    "for current_order in order_ids:\n",
    "\n",
    "    order_idx = df_order[df_order[\"order_id\"] == current_order].index[0]\n",
    "    sellers_id_list = list(df_order[df_order[\"order_id\"] == current_order][\"seller_list\"].values[0])\n",
    "    lat_lon_list = []\n",
    "\n",
    "    for seller_id in sellers_id_list:\n",
    "        seller_row = df_sellers[df_sellers[\"seller_id\"] == seller_id]\n",
    "        lat, lon = float(seller_row[\"lat\"].values[0]), float(seller_row[\"lon\"].values[0])\n",
    "        lat_lon = (lat, lon)\n",
    "        lat_lon_list.append(lat_lon)\n",
    "        del lat_lon\n",
    "    df_order.at[order_idx, \"geoloc_sellers_list\"] = lat_lon_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Thats nice we can work with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> 0.3 : Reviews for all orders :</u>\n",
    "\n",
    "### How :\n",
    "- Similar method to above, only with df_reviews\n",
    "- Selecting what we want to know from review with df.head()\n",
    "<br>\n",
    "\n",
    "> <i>(note to self : work on a generic way of approaching the simple problem (not the list of lists) with concurrent.futures) </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets select our variables first :\n",
    "\n",
    "df_reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We are mainly interested in review score, but we can also create a boolean variable to check whether or not the review was assorted with a comment or not. We can debate whether or not the lenght of the review is important. In the future we can also think about using NLP engines such as GPT-3 to detect the \"tone\" of the answer and the topics approached. This would be, however, a little heavy for the model and a lot a work to put into place\n",
    "\n",
    "<br>\n",
    "\n",
    "<b>For now</b>\n",
    "\n",
    "- Create the boolean value for comment in reviews directly\n",
    "- Create a new boolean assign boolean values : has_rating and has_comment\n",
    "- <b>Maybe</b> (check with team) create a boolean value to check if, in the case of comment, an Olist support agent has replied. Could be relevant (we like to know that our comment was read or our complaint heard/taken care of). It isnt explicit on the dataset if this is a support agent who replied or the Cx who updated the review. Will check online\n",
    "\n",
    "<i><b>!!!Note!!!</b> : after online (Kaggle, cf. sources) check : review_answer_timestamp : Shows satisfaction survey answer timestamp., no support agent involved as far as I can see. Last point is irrelevant but could be a nice addition for Olist's databases in the future.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_comment(row) -> bool:\n",
    "    \"\"\"\n",
    "    - Determines if a review is assorted with a comment/message (returns boolean)\n",
    "    - For dataset : reviews\n",
    "    \"\"\"\n",
    "\n",
    "    if not pd.isna(row[\"review_comment_message\"]):\n",
    "        return True\n",
    "    elif pd.isna(row[\"review_comment_message\"]):\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop the non useful columns :\n",
    "df_reviews.drop(columns=[\"review_answer_dt\", \"review_creation_date\", \"review_comment_title\"], inplace=True)\n",
    "\n",
    "df_reviews.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the ratings as barplot out of curiosity :\n",
    "In general, only two kind of people rate/comment : the really happy ones and the really disappointed ones. If this is true here, we should see a disproportional amount of good (5) and bad (1, 2), a little bit of 4s but a very low amount of 3s. Let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcing dtypes : \n",
    "df_reviews[\"review_score\"] = df_reviews[\"review_score\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmap = {1: \"#810000\", 2: \"red\", 3: \"royalblue\", 4: \"navy\", 5: \"#003153\"}\n",
    "title = \"Distribution of ratings in the reviews given by users\"\n",
    "\n",
    "fig = px.histogram(df_reviews, x=\"review_score\", color=\"review_score\", color_discrete_map=cmap, title=title)\n",
    "\n",
    "fig.update_layout(margin={\"b\": 25, \"t\": 25, \"r\": 25, \"l\": 25})\n",
    "fig.update_layout(title_y=0.95, title_x=0.1)\n",
    "fig.update_layout(font_color=\"black\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's place our boolean in review : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"has_comment\"] = df_reviews.apply(has_comment, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"review_score\"] = np.dtype(\"int\")\n",
    "df_order[\"has_rating\"] = np.dtype(\"bool\")\n",
    "df_order[\"has_comment\"] = np.dtype(\"bool\")\n",
    "\n",
    "order_ids = df_order[\"order_id\"].values\n",
    "\n",
    "for current_order in order_ids:\n",
    "    order_idx = df_order[df_order[\"order_id\"] == current_order].index[0]\n",
    "    try:\n",
    "        review = df_reviews[df_reviews[\"order_id\"] == current_order]\n",
    "        df_order.at[order_idx, \"review_score\"] = review[\"review_score\"].values[0]\n",
    "        df_order.at[order_idx, \"has_rating\"] = True\n",
    "        if any(review[\"has_comment\"]):\n",
    "            df_order.at[order_idx, \"has_comment\"] = True\n",
    "        else:\n",
    "            df_order.at[order_idx, \"has_comment\"] = False\n",
    "\n",
    "    except IndexError:\n",
    "        df_order.at[order_idx, \"review_score\"] = np.nan\n",
    "        df_order.at[order_idx, \"has_rating\"] = False\n",
    "        df_order.at[order_idx, \"has_comment\"] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <u>1 : Calculate Time deltas for orders</u>\n",
    "## 1.1 : Calculate Time delta of delivery for delivered orders (status = delivered)\n",
    "## 1.2 : Calculate Time delta of expected/delivered orders + booleans\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>1.1 : Calculate Time delta of delivery for delivered orders (status = delivered) </u>\n",
    "\n",
    "### 1 : How ?\n",
    "We apply only that ∆ on delivered orders, if the order[\"order_delivered_customer_date\"] is blank, function will skip it\n",
    "### 2 : Applied to Cx :\n",
    "We will calculate the average ∆T for each Cx using the list of orders in df_rfm : for Cx with exactly one order, this will be of no consequence as ∆T will be from only one order, but for Cxs w/ multiple orders, we will take the avg. ∆. For Cx w/ no ∆ present on orders (if their order status is not \"deliverd\"), see point 3.\n",
    "### 3 : Remaining values : \n",
    "No modification of missing values until the merger with the rfm dataset -- The merger then will use a mean inputer while taking into account the cluster of the customer\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkings for NAs in \"order_delivered_customer_date\" & \"order_purchase_dt\"\n",
    "temp = df_order[df_order[\"order_status\"] == \"delivered\"]\n",
    "\n",
    "delivered_na_count = temp[\"order_delivered_customer_date\"].isna().sum()\n",
    "purchase_dt_na_count = temp[\"order_purchase_dt\"].isna().sum()\n",
    "\n",
    "print(f\"There is {delivered_na_count} values missing in order_delivered_customer_date\")\n",
    "print(f\"There is {purchase_dt_na_count} values missing in order_purchase_dt\")\n",
    "\n",
    "del temp  # Flush\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 values missing in order_delivered_customer_date, likely the order was delivered too recently at sql dump time. Regardless, ignoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_t(row):\n",
    "    \"\"\"\n",
    "    If status of order is delivered, returns delta between the delivery and order.\n",
    "    If any data is missing for this operation to work, returns nan\n",
    "    \"\"\"\n",
    "    if row[\"order_status\"] == \"delivered\":\n",
    "\n",
    "        delivered_at = pd.to_datetime(row[\"order_delivered_customer_date\"])\n",
    "        ordered_at = pd.to_datetime(row[\"order_purchase_dt\"])\n",
    "        \n",
    "        if pd.isna(delivered_at) or pd.isna(ordered_at):\n",
    "            return np.nan\n",
    "        elif not pd.isna(delivered_at) and not pd.isna(ordered_at):\n",
    "            return delivered_at - ordered_at\n",
    "    else:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"delta_delivery\"] = df_order.apply(calc_delta_t, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking :\n",
    "\n",
    "df_order[\"delta_delivery\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking number of nans and the number of not delivered orders, should be pretty similar : \n",
    "\n",
    "na_delta_del = df_order['delta_delivery'].isna().sum()\n",
    "not_delivered_len = len(df_order[df_order['order_status'] != 'delivered'])\n",
    "print(f\"{na_delta_del} missing values in Delta Delivery\")\n",
    "print(f\"{not_delivered_len} missing values in packages not delivered\")\n",
    "\n",
    "print(f\"delta_del_na - not_delivered = {na_delta_del - not_delivered_len}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the same, expected number of delivered packages without a delta delivery. As predicted\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> 1.2 : Calculate Time delta of expected/delivered orders + booleans </u>\n",
    "\n",
    "- Goals : We want to establish, for packages delivered, the difference between the expected date of delivery and the actual delivery date.<br>\n",
    "It might be a good indicator of satisfaction : we are happy if our package is early/on time, and not happy if we are delivered late.\n",
    "- Creation of booleans : EarlyOrOnTime : True for early and on time , False for late\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_expect_delta(row):\n",
    "    \"\"\"\n",
    "    Calculates the time delta between the expected delivery date, keeping y, m, d in order_delivered_customer_date\n",
    "    3 cases if not nan:\n",
    "    - value = 0 --> On time\n",
    "    - value < 0 --> Early\n",
    "    - Value > 0 --> Late\n",
    "\n",
    "    Returns : pd.TimeDelta or np.nan\n",
    "    \"\"\"\n",
    "\n",
    "    if row[\"order_status\"] == \"delivered\":\n",
    "\n",
    "        delivered_at_full = pd.to_datetime(row[\"order_delivered_customer_date\"])\n",
    "        expected_at = pd.to_datetime(row[\"order_estimated_delivery_date\"])\n",
    "\n",
    "        if pd.isna(delivered_at_full) or pd.isna(expected_at):\n",
    "            return np.nan\n",
    "        elif not pd.isna(delivered_at_full) and not pd.isna(expected_at):\n",
    "        \n",
    "            delivered_at_trunc = pd.to_datetime(\n",
    "                datetime.date(\n",
    "                        int(delivered_at_full.year),\n",
    "                        int(delivered_at_full.month),\n",
    "                        int(delivered_at_full.day)\n",
    "                    )\n",
    "                )\n",
    "            return delivered_at_trunc - expected_at\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def on_time_late(row):\n",
    "    \"\"\"\n",
    "    Returns True if package arrived before or on the predicted day, and False if package is late\n",
    "    \"\"\"\n",
    "\n",
    "    if row[\"order_status\"] == \"delivered\":\n",
    "        delta_days = row['expectation_delta'].days\n",
    "        if not pd.isna(row[\"expectation_delta\"]):\n",
    "            if delta_days <= 0:\n",
    "                return True\n",
    "            elif delta_days > 0:\n",
    "                return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"expectation_delta\"] = df_order.apply(calc_expect_delta, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = df_order[\"expectation_delta\"].value_counts().to_dict()\n",
    "deltas = dict(sorted(deltas.items()))\n",
    "\n",
    "delta_days = {}\n",
    "\n",
    "for key, value in deltas.items():\n",
    "    delta_days[key.days] = value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have a lot of extremes. Plotting in bars to see clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 3),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.boxplot(x=delta_days, vert=False, showfliers=True, showmeans=True, widths=.5)\n",
    "\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_xlabel(\"Delta between expected and actual delivery\")\n",
    "fig.suptitle(\"Repartition of deltas between expected and actual delivery\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This repartition shows that there is an unusual amount of very high Deltas : -150, +180, these are not normal and might not be what is expected but edge cases to view case by case by Olist, knowing the context might explain these values. Nevertheless, there are the values the dataset gives us, thus, this is the data we will use. Theory is that date format was inversed (either DDMMYYYY became MMDDYYYY or the other way around) - We will discard these depending on quantile representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean and we're done :\n",
    "\n",
    "df_order[\"early_on_time\"] = df_order.apply(on_time_late, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"expectation_delta\"].describe(percentiles=([.001, .1, .25, .75, .9, .95, .998]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking up the quantiles, 0.1%, 99.9%\n",
    "> We will discard these extreme cases. (above 0.1% and under 99.9%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_0_1 = df_order[\"expectation_delta\"].quantile(0.001) # Getting the value @ 0.1%\n",
    "p_99_9 = df_order[\"expectation_delta\"].quantile(0.999) # Getting the value @ 99.9%\n",
    "\n",
    "remove_low = df_order[df_order[\"expectation_delta\"].lt(p_0_1) & df_order[\"expectation_delta\"].notna()].index.tolist()\n",
    "remove_high = df_order[df_order[\"expectation_delta\"].gt(p_99_9) & df_order[\"expectation_delta\"].notna()].index.tolist()\n",
    "\n",
    "remove_high_low = remove_high + remove_low\n",
    "\n",
    "print(f\"We will remove {len(remove_high_low)} orders from the dataframe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = df_order.drop(index=remove_high_low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redoing the index\n",
    "\n",
    "df_order.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking- check True :\n",
    "\n",
    "df_order[df_order[\"early_on_time\"] == True].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking- check na :\n",
    "\n",
    "df_order[df_order[\"early_on_time\"].isna()].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking- check False :\n",
    "\n",
    "df_order[df_order[\"early_on_time\"] == False].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <u>2 : Caculate Dist delta of seller - buyers</u>\n",
    "\n",
    "> Using a function that calculates the distance between two points on Earth using the Haversine formula, we can :\n",
    "    > - Get the ∆ distance between a seller and a buyer\n",
    "    > - If there are more than one sellers, calculating the average distance\n",
    "    > - If there are no sellers, returning nans\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_dist(row):\n",
    "    \"\"\"\n",
    "    Uses structural pattern matching and haversine_km function to calculate :\n",
    "    - case nb_seller = 1 : the delta dist between seller and cx\n",
    "    - case nb_seller != 1 and != 0 (case _) : total_delta_dist / nb_sellers (avg. dist)\n",
    "    - case nb_seller = 0 : returns np.nan\n",
    "    \"\"\"\n",
    "\n",
    "    lat_cx, lon_cx = row[\"geoloc_cx\"][0], row[\"geoloc_cx\"][1]\n",
    "    seller_list = row[\"geoloc_sellers_list\"]\n",
    "    match len(seller_list):\n",
    "        case 1:\n",
    "            seller_tuple = seller_list[0]\n",
    "            seller_lat, seller_lon = seller_tuple[0], seller_tuple[1]\n",
    "            return haversine_km(\n",
    "                lat_o=lat_cx,\n",
    "                lon_o=lon_cx,\n",
    "                lat_f=seller_lat,\n",
    "                lon_f=seller_lon\n",
    "            )\n",
    "\n",
    "        case 0:\n",
    "            return np.nan\n",
    "\n",
    "        case _:\n",
    "            seller_nb = len(seller_list)\n",
    "            delta_total = 0\n",
    "            for seller_tuple in seller_list:\n",
    "                seller_lat, seller_lon = seller_tuple[0], seller_tuple[1]\n",
    "                delta_total += haversine_km(\n",
    "                    lat_o=lat_cx,\n",
    "                    lon_o=lon_cx,\n",
    "                    lat_f=seller_lat,\n",
    "                    lon_f=seller_lon)\n",
    "            avg_delta = delta_total / seller_nb\n",
    "            return avg_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order[\"seller_to_cx_dist\"] = df_order.apply(get_delta_dist, axis=1)\n",
    "\n",
    "df_order.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <u>3 : Transitionning from the orders dataset to the cx_rfm dataset.</u>\n",
    "\n",
    "&emsp;This will enable us to add all the new variables to the first RFM classification. We will need to ensure that the merger is done correctly, and, since we deleted 191 orders from the dataset, we might need to filter some Cxs. For multiple orders, we will need to choose between keeping multiple orders or doing the average of the variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>3.1: Reviews.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>3.1.1: Reviews, cases not NA.</u>\n",
    "\n",
    "- Review Score : Average for each individual Cx if multiple orders\n",
    "- Has Rated : Boolean, at least once\n",
    "- Rating Ratio : Orders Rated / Total Orders made\n",
    "- Commented : At least once\n",
    "- Comment Ratio : Orders commented / Total Orders made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_columns = df_order.columns.tolist()\n",
    "\n",
    "print(order_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep/Discard : \n",
    "<hr>\n",
    "\n",
    "##### Discard :\n",
    "- `order_purchase_dt', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date'` are already partially in df_rfm, no use in duplication\n",
    "- `geoloc_cx', 'geoloc_sellers_list'` : case Cx, already computed the most recent known GPS position, no use for sellers.\n",
    "\n",
    "##### Method : \n",
    "We will use `order_id_list` and loop over the rfm dataset, taking the info we need. To reduce computation time, we first will remove columns we don't need.\n",
    "\n",
    "##### Maybe :\n",
    "The use of products can be important, we will append it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m for merger\n",
    "columns_drop = [\n",
    "    \"order_purchase_dt\", \"order_approved_at\", \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\", \"order_estimated_delivery_date\", \"geoloc_cx\",\n",
    "    \"geoloc_sellers_list\"\n",
    "    ]\n",
    "\n",
    "df_order_m = df_order.drop(columns=columns_drop)\n",
    "\n",
    "df_order_m.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.dtypes\n",
    "\n",
    "# Type enforcement and export as pkl necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm[\"customer_uid\"] = df_rfm[\"customer_uid\"].astype(np.uint32)\n",
    "df_rfm[\"num_orders\"] = df_rfm[\"num_orders\"].astype(np.uint8)\n",
    "df_rfm[\"cluster_kmeans_4\"] = df_rfm[\"cluster_kmeans_4\"].astype(np.uint8)\n",
    "df_rfm[\"cluster_DBSCAN\"] = df_rfm[\"cluster_DBSCAN\"].astype(np.uint8)\n",
    "df_rfm[\"most_ancient_order_dt\"] = pd.to_datetime(df_rfm[\"most_ancient_order_dt\"])\n",
    "df_rfm[\"most_recent_order_dt\"] = pd.to_datetime(df_rfm[\"most_recent_order_dt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Somewhere along the way I broke the order lists so lets fix,\n",
    "# # I dont know where, but this works as a fix\n",
    "# # Lets comment that out once we are done, saves time \n",
    "# # (there are better ways to spend 2 minutes of computing power)\n",
    "# # Uncomment if orderlists are broken again\n",
    "\n",
    "# unique_ids = df_rfm[\"customer_uid\"].unique()\n",
    "\n",
    "# uid_aliases = dict.fromkeys(unique_ids)\n",
    "\n",
    "# for uid in uid_aliases:\n",
    "#     uid_aliases[uid] = df_cx[df_cx[\"customer_unique_id\"] == uid][\"customer_id\"].values.tolist()\n",
    "\n",
    "# # Drop\n",
    "# try:\n",
    "#     df_rfm.drop(columns=[\"order_id_list\"], inplace=True)\n",
    "# except KeyError:\n",
    "#     #means we dropped it\n",
    "#     pass\n",
    "\n",
    "\n",
    "# rfm_id_drop = []  # As long as we are here we can pick up the 192 deleted cxs\n",
    "\n",
    "# df_rfm[\"order_id_list\"] = np.dtype(\"object\")\n",
    "\n",
    "# for key, alias_list in uid_aliases.items():\n",
    "#     rfm_index = df_rfm[df_rfm[\"customer_uid\"] == key].index.values[0]\n",
    "#     order_list = []\n",
    "#     for alias in alias_list:\n",
    "#         try:\n",
    "#             order_by_alias = df_order[df_order[\"customer_id\"] == alias][\"order_id\"].values\n",
    "#             order_list.append(int(order_by_alias))\n",
    "#         except TypeError:\n",
    "#             # Means we are on one of the 192 deleted orders, thats fine, skip\n",
    "#             pass\n",
    "#     if len(order_list) >= 1:\n",
    "#         df_rfm.at[rfm_index, \"order_id_list\"] = np.array(order_list, dtype=object)\n",
    "#     else:\n",
    "#         rfm_id_drop.append(rfm_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving in the middle because of the lists, uncomment to redo\n",
    "# df_rfm.to_pickle(path=\"../pickles/geolocd/cx_rfm.plk\")\n",
    "# df_rfm.to_csv(path_or_buf=\"../data/optimized/geoloc_applied/cx_rfm.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm[\"rating_avg\"] = np.nan\n",
    "df_rfm[\"rating_ratio\"] = np.nan\n",
    "df_rfm[\"comment_ratio\"] = np.nan\n",
    "df_rfm[\"has_rated\"] = False\n",
    "df_rfm[\"has_commented\"] = False\n",
    "\n",
    "reviews_fields = [\"rating_avg\", \"rating_ratio\", \"comment_ratio\", \"has_rated\", \"has_commented\"]\n",
    "Reviews_info = namedtuple(typename=\"Reviews_info\", field_names=reviews_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_info(id_list):\n",
    "\n",
    "    orders = df_order_m[df_order_m[\"order_id\"].isin(id_list)]\n",
    "    match len(orders):\n",
    "\n",
    "        case 1:\n",
    "            if any(orders[\"review_score\"].notna()):\n",
    "                rating = orders[\"review_score\"].values.sum()\n",
    "                rating_ratio = 1\n",
    "                has_rated = True\n",
    "            else:\n",
    "                rating = np.nan\n",
    "                rating_ratio = 0\n",
    "                has_rated = False\n",
    "\n",
    "            if any(orders[\"has_comment\"] == True):\n",
    "                has_commented = True\n",
    "                comment_ratio = 1\n",
    "            else:\n",
    "                has_commented = False\n",
    "                comment_ratio = 0\n",
    "\n",
    "        case _:\n",
    "            if any(orders[\"review_score\"].notna()):\n",
    "                avg_orders = orders.dropna(subset=[\"review_score\"])\n",
    "                rating = np.average(avg_orders[\"review_score\"].values.tolist())\n",
    "                has_rated = True\n",
    "                rating_ratio = len(orders[\"review_score\"].notna()) / len(orders)\n",
    "            else:\n",
    "                rating = np.nan\n",
    "                has_rated = False\n",
    "                rating_ratio = 0\n",
    "\n",
    "            if any(orders[\"has_comment\"] == True):\n",
    "                comment_count = len(orders[orders[\"has_comment\"]] == True)\n",
    "                comment_ratio = comment_count / len(orders)\n",
    "                has_commented = True\n",
    "            else:\n",
    "                comment_ratio = 0\n",
    "                has_commented = False\n",
    "\n",
    "    reviews_info = Reviews_info(\n",
    "        rating_avg=rating, rating_ratio=rating_ratio,\n",
    "        comment_ratio=comment_ratio, has_rated=has_rated, has_commented=has_commented\n",
    "        )\n",
    "    return reviews_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_all = df_order[\"order_id\"].unique()\n",
    "rm_rfm_idx = []\n",
    "\n",
    "# Takes a long while, finding a more efficient way would be an improvement\n",
    "\n",
    "for index, row in df_rfm.iterrows():\n",
    "    rfm_order_ids = row[\"order_id_list\"]\n",
    "    order_in_both = False\n",
    "    try:\n",
    "        reviews_info = get_review_info(rfm_order_ids)\n",
    "        df_rfm.at[index, \"rating_avg\"] = reviews_info.rating_avg\n",
    "        df_rfm.at[index, \"rating_ratio\"] = reviews_info.rating_ratio\n",
    "        df_rfm.at[index, \"comment_ratio\"] = reviews_info.comment_ratio\n",
    "        df_rfm.at[index, \"has_rated\"] = reviews_info.has_rated\n",
    "        df_rfm.at[index, \"has_commented\"] = reviews_info.has_commented\n",
    "    except TypeError:\n",
    "        rm_rfm_idx.append(index)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"removing {len(rm_rfm_idx)} clients since we removed their orders duringthe percentile filtering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's give it a new name since its no longer RFM\n",
    "\n",
    "df_export = df_rfm.drop(index=rm_rfm_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>3.2 Delta delivery & Delta Expected/Actual</u>\n",
    "\n",
    "- ∆Delivery : Average if multiple orders, nan if no orders (?) or input cluster avg.\n",
    "- ∆Expect/delivery : Average if multiple orders, nan if no orders (?) or input cluster avg.\n",
    "- Early_on_time : True majority, including 50% , False if under, Nan if no info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export[\"delta_delivery\"] = np.nan\n",
    "df_export[\"expected_reality\"] = np.nan\n",
    "df_export[\"early_on_time\"] = np.nan\n",
    "\n",
    "df_export.reset_index(drop=True, inplace=True)\n",
    "\n",
    "distance_fields = [\"delta_delivery\", \"expected_reality\", \"early_on_time\"]\n",
    "\n",
    "Distance_info = namedtuple(\"Distance_info\", field_names=distance_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_info(order_list):\n",
    "    orders = df_order_m[df_order_m[\"order_id\"].isin(order_list)]\n",
    "    \n",
    "    match len(orders):\n",
    "    \n",
    "        case 1:\n",
    "            delta_delivery = orders[\"delta_delivery\"].values[0]\n",
    "            expected_reality = orders[\"expectation_delta\"].values[0]\n",
    "            early_on_time = orders[\"early_on_time\"].values[0]\n",
    "    \n",
    "        case _:\n",
    "            try:\n",
    "                delta_delivery = np.average(orders[\"delta_delivery\"].values)\n",
    "                expected_reality = np.average(orders[\"delta_delivery\"].values)\n",
    "                trues_num = len(orders[orders[\"early_on_time\"] == True])\n",
    "                false_num = len(orders[orders[\"early_on_time\"] == False])\n",
    "            \n",
    "                if trues_num >= false_num:\n",
    "                    early_on_time = True\n",
    "                elif false_num > trues_num:\n",
    "                    early_on_time = False\n",
    "\n",
    "            except ValueError:\n",
    "                distance_info = Distance_info(\n",
    "                    delta_delivery=np.nan,\n",
    "                    expected_reality=np.nan,\n",
    "                    early_on_time=np.nan\n",
    "                    )\n",
    "                return distance_info\n",
    "\n",
    "    distance_info = Distance_info(\n",
    "        delta_delivery=delta_delivery,\n",
    "        expected_reality=expected_reality,\n",
    "        early_on_time=early_on_time\n",
    "        )\n",
    "\n",
    "    return distance_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_export.iterrows():\n",
    "    export_orders = row[\"order_id_list\"]\n",
    "    order_in_both = False\n",
    "    distance_info = get_time_info(export_orders)\n",
    "    df_export.at[index, \"delta_delivery\"] = distance_info.delta_delivery\n",
    "    df_export.at[index, \"expected_reality\"] = distance_info.expected_reality\n",
    "    df_export.at[index, \"early_on_time\"] = distance_info.early_on_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>3.3 : Deltas distances Cx/seller :</u>\n",
    "- Average if multiple orders\n",
    "- Checking for cases with no orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export[\"distance_cx_seller\"] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_info(order_list):\n",
    "    orders = df_order_m[df_order_m[\"order_id\"].isin(order_list)]\n",
    "    match len(orders):\n",
    "        case 1:\n",
    "            return float(orders[\"seller_to_cx_dist\"])\n",
    "        case _:\n",
    "            return np.average(orders[\"seller_to_cx_dist\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_export.iterrows():\n",
    "    order_ids = row[\"order_id_list\"]\n",
    "    delta_dist = get_dist_info(order_list=order_ids)\n",
    "    df_export.at[index, \"distance_cx_seller\"] = delta_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exporting we want to ensure correct types and formats are enforced, Timedeltas have switched from days to nanoseconds and this is an unnecessary degree of precision we corrected earlier.\n",
    "Early/OnTime is boolean with some NaNs - the rest looks okay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export[\"delta_delivery\"] = pd.to_timedelta(df_export[\"delta_delivery\"], unit=\"days\")\n",
    "df_export[\"expected_reality\"] = pd.to_timedelta(df_export[\"expected_reality\"], unit=\"days\")\n",
    "df_export[\"early_on_time\"] = df_export[\"early_on_time\"].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good and usable, let's export that as pickle and csv for backup\n",
    "\n",
    "df_export.to_pickle(path=\"../final_datasets/olist_customers.pkl\")\n",
    "df_export.to_csv(path_or_buf=\"../final_datasets/olist_customers.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2214c6fb926f71468a18ca7cf946224dc61b1c77c0f95ec2e3a3e68abe6a5ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
