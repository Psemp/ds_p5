{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "data_path = \"../data\"\n",
    "\n",
    "from models.cx_groups import Cx_groups\n",
    "from models.easy_pca import Easy_pca\n",
    "from scripts.optimizers_mp import k_means_optimizer\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "load_dotenv()\n",
    "sns.color_palette('colorblind')\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "try:\n",
    "    pc_dpi = int(os.getenv('DPI'))\n",
    "except TypeError:\n",
    "    pc_dpi = 100\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>Taking into account what RFM approach taught us and the clusters we extracted from the data :</i>\n",
    "\n",
    "# 1 : Statistics considering the clusters obtained via RFM\n",
    "# 2 : Delta Order / Delivery and its effects on customers' satisfaction.\n",
    "# 3 : Delta dist, is it a determining factor in the choice made by clients ?\n",
    "# 4 : Geolocation, are our customers more in cities etc.\n",
    "# 5 : PCA on selected variables for classification\n",
    "# 6 : Conclusions, modelisations of adjusted parameters and exports\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olist_clients_dataset = \"../final_datasets/olist_customers.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx = pd.read_pickle(olist_clients_dataset)\n",
    "\n",
    "df_cx.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <u>1 : Statistics considering the clusters obtained via RFM.</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>1.1 : Ratings across all the dataset:</u>\n",
    "\n",
    "<b><u>Goals : </u></b>\n",
    "\n",
    "&emsp;This will help to visualize how many people have left at least one Review, and if they commented it. We will see the involvment of customers, first across all the dataset. We call calculate the avg of all ratings to provide a reference/comparison point for ratings per cluster\n",
    "\n",
    "<b><u>Method : </u></b>\n",
    "\n",
    "&emsp;We will first assess the ammount of customers that have rated and/or commented. We will calculate the average ratio for ratings and comments. It will be interesting to see, first, those metrics on the dataset, then applied to our first successful segmentation (k-means w/ k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if people can comment without rating : \n",
    "\n",
    "commenters = df_cx[df_cx[\"has_commented\"] == True]\n",
    "print(f\"{len(commenters[commenters['rating_avg'] == np.nan])}\")\n",
    "\n",
    "del commenters  # Flush\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addlabels(x, y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], f\"{y[i]}\", ha=\"center\", bbox=dict(facecolor=\"white\", alpha=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No they cant, okay so cx who have posted a comment have to rate the order.\n",
    "\n",
    "ratings_avg_dswide = np.average(df_cx[df_cx[\"rating_avg\"].notna()][\"rating_avg\"].values.tolist())\n",
    "ratings_ratio_dswide = np.average(df_cx[\"rating_ratio\"])\n",
    "comment_ratio_dswide = np.average(df_cx[\"comment_ratio\"])\n",
    "\n",
    "rating_poster_dswide = len(df_cx[df_cx[\"has_rated\"] == True])\n",
    "comment_poster_dswide = len(df_cx[df_cx[\"has_commented\"] == True])\n",
    "no_rating = len(df_cx[df_cx[\"has_rated\"] == False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(10, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "bars = {\"rating_poster\": rating_poster_dswide, \"comment_and_rate\": comment_poster_dswide, \"no_review\": no_rating}\n",
    "\n",
    "cmap_one = [\"#000331\", \"navy\", \"red\"]\n",
    "\n",
    "ax1.bar(x=list(bars.keys()), height=list(bars.values()), color=cmap_one)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "addlabels(x=[cat for cat in bars.keys()], y=[pop for pop in bars.values()])\n",
    "ax1.set_ylabel(\"Population of group\")\n",
    "ax1.set_xlabel(\"Group\")\n",
    "ax1.tick_params(left=False)\n",
    "fig.suptitle(\"Amount of users by their method of rating\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;By design (Olist sharing mostly orders which have been subject to customer review) or by accident (very small non zero chance), the crushing majority of customers have rated their orders at least once (95211 out of 95922, or about 99.26% of customers).\n",
    "\n",
    "&emsp;There is also a considerable amount of customers who have assorted their reviews with a comment (You need to rate the order to comment it, at least, in this dataset, no commented order was unrated) : 39700 out of 95922 clients, which is about 41.39% of the total population.\n",
    "\n",
    "&emsp;Finally, a crushing minority of clients (711, which represents 0.74% of the dataset) have neither rated nor commented any of their orders. Due to the nature of the data, we can theorize that this concerns orders that have not been yet completed (delivered ?) at the time of SQL dump. We will include this in our report and inquire about those 711 customers.\n",
    "\n",
    "<hr>\n",
    "\n",
    "&emsp;We need to visualize at which ratio customers have reviewed and commented in average, but since most (but not all) customers have only placed one order, we can expect this number to quite close to 1, for ratings at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(10, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "bars_ratios_dswide = {\"review_ratio\": ratings_ratio_dswide, \"comment_ratio\": comment_ratio_dswide}\n",
    "\n",
    "cmap_one = [\"#000331\", \"navy\"]\n",
    "\n",
    "ax1.bar(x=list(bars_ratios_dswide.keys()), height=list(bars_ratios_dswide.values()), color=cmap_one)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "addlabels(x=[cat for cat in bars_ratios_dswide.keys()], y=[pop for pop in bars_ratios_dswide.values()])\n",
    "ax1.set_ylabel(\"Ratio (0-1)\")\n",
    "ax1.tick_params(left=False)\n",
    "fig.suptitle(\"Average rating and comment ratio, datasetwide\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;That was expected, this is in line with what we saw on the previous chart. Average ratio of ratings is close to 1 and comment ratio close to 0.4139 (our 41.39% from the comment group earlier).\n",
    "\n",
    "&emsp;We can conclude this analysis of the review ratio by stating that, being that close to 1 and seemingly almost mandatory for the order to be in the dataset, is of no use since it cannot efficiently help our clustering. We later drop this variable (keeping the boolean for now), but we suggest its use in the model chosen at the end of this analysis. According to this article : <a href=\"https://www.gwi.com/hubfs/Downloads/Brand_Discovery-2019.pdf\">this article</a>, in 2019, 47% of ecommerce customers leave a review each month. It would be necessary to see if that propotion is similar in the whole olist database or it if keeps being sky high.\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "&emsp;Let's take a look at the rating distribution in the dataset and the average rating left by customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rounding up to 1/1.5/2/2.5/3/3.5/4/4.5/5\n",
    "rounded_list = []\n",
    "\n",
    "og_list = df_cx[df_cx[\"rating_avg\"].notna()][\"rating_avg\"].values.tolist()\n",
    "\n",
    "for rating in og_list:\n",
    "    rounded_list.append(round(rating * 2) / 2)\n",
    "\n",
    "uniques = []\n",
    "for rating in rounded_list:\n",
    "    if rating not in uniques:\n",
    "        uniques.append(rating)\n",
    "\n",
    "uniques.sort()\n",
    "\n",
    "print(uniques)\n",
    "\n",
    "# Nice, feels like im reinventing the wheels there but that works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dswide = dict.fromkeys(uniques)\n",
    "\n",
    "for rating in ratings_dswide:\n",
    "    ratings_dswide[rating] = rounded_list.count(rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(10, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "colors = [\"black\", \"dimgray\", \"darkred\", \"red\", \"yellow\", \"steelblue\", \"royalblue\", \"navy\", \"#000331\"]\n",
    "\n",
    "ratings = [str(number) for number in np.arange(1, 5.5, 0.5)]\n",
    "amount = rating\n",
    "ax1.bar(x=ratings, height=list(ratings_dswide.values()), color=colors)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "addlabels(x=[rating for rating in ratings_dswide.keys()], y=[pop for pop in ratings_dswide.values()])\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;Looks like there is an overwhelming number of very high ratings (5 = half of the customers). This looks overly optimistic but it is not unlikely as the study quoted earlier states that satisfied customers are more likely to give a review. Negative ones too but less so, <a href=\"https://findstack.com/online-review-statistics/\">according to this study</a> (point 25).\n",
    "\n",
    "&emsp;So we can expect the average overall rating to be quite high. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average for the whole dataset = {ratings_avg_dswide}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have our reference value, now we can use it as a baseline to determine clusters characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>1.2 : Statistics using the clusters attributed by k-means with k=4:</u>\n",
    "\n",
    "<b><u>Goals :</u></b>\n",
    "\n",
    "&emsp;We might be able to distinguish different characteristics from cluster to cluster, hence giving us a way to compare and evaluate the relevance of a given variable. (Which could otherwise be done using SHAP)\n",
    "\n",
    "<b><u>Method :</u></b>\n",
    "\n",
    "&emsp;For starters, Let's use names of clusters and not their numbers. It will make more sense to talk about people and not numbers.\n",
    "\n",
    "<br>\n",
    "\n",
    "We will use a radar plot to reresent the most important statistics and their average per group :\n",
    "- Delta Days\n",
    "- Average Rating\n",
    "- Delta Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min Max Scaler doesnt support timedeltas, so we need to convert timedeltas to ints (days)\n",
    "\n",
    "def get_delta_days(row):\n",
    "    try:\n",
    "        return int(row[\"expected_reality\"].days)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx[\"delta_days\"] = df_cx.apply(get_delta_days, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler(feature_range=(0, 5))\n",
    "\n",
    "subset = [\"rating_avg\", \"delta_days\", \"distance_cx_seller\", \"recency\", \"frequency\"]\n",
    "scaled_subset = [\n",
    "    \"scaled_rating_avg\", \"scaled_delta_days\",\n",
    "    \"scaled_distance_cx_seller\", \"scaled_recency\",\n",
    "    \"scaled_frequency\"\n",
    "    ]\n",
    "\n",
    "keepcols = [\"rating_avg\", \"delta_days\", \"distance_cx_seller\", \"recency\", \"frequency\", \"k_cluster_name\"]\n",
    "\n",
    "df_cx_mms = df_cx[keepcols].copy()\n",
    "\n",
    "df_cx_mms[scaled_subset] = mms.fit_transform(df_cx_mms[subset].to_numpy())\n",
    "\n",
    "df_cx_mms.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investor_group = Cx_groups(cluster_name=\"investors\", dataframe=df_cx_mms[df_cx_mms[\"k_cluster_name\"] == \"investors\"])\n",
    "early_m_group = Cx_groups(cluster_name=\"early_majority\", dataframe=df_cx_mms[df_cx_mms[\"k_cluster_name\"] == \"early_majority\"])\n",
    "late_m_group = Cx_groups(cluster_name=\"late_majority\", dataframe=df_cx_mms[df_cx_mms[\"k_cluster_name\"] == \"late_majority\"])\n",
    "lagg_group = Cx_groups(cluster_name=\"laggards\", dataframe=df_cx_mms[df_cx_mms[\"k_cluster_name\"] == \"laggards\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_inv = investor_group.get_radar()\n",
    "em_radar = early_m_group.get_radar()\n",
    "lm_radar = late_m_group.get_radar()\n",
    "lagg_radar = lagg_group.get_radar()\n",
    "\n",
    "# Let's superpose the radars : (Class Cx Group saves the trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(investor_group.trace))\n",
    "fig.add_trace(go.Scatterpolar(early_m_group.trace))\n",
    "fig.add_trace(go.Scatterpolar(late_m_group.trace))\n",
    "fig.add_trace(go.Scatterpolar(lagg_group.trace))\n",
    "\n",
    "colors = [\"#000331\", \"navy\", \"royalblue\", \"red\"]\n",
    "\n",
    "title = \"Newly created stats, MinMaxed for each cluster determined during RFM segmentation Each cluster\"\n",
    "\n",
    "fig.update_layout(\n",
    "  title=title,\n",
    "  polar=dict(\n",
    "    radialaxis=dict(\n",
    "      visible=True,\n",
    "      range=[0, 5]\n",
    "    )),\n",
    "  showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investor_group.get_standard_stats()\n",
    "investor_group.standard_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagg_group.get_standard_stats()\n",
    "lagg_group.standard_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;As we can see here and in the plots above, the ratings are unexpectedly high and the standard deviation very low. On our sample, customers are overall satisfied/very satisfied. This behavious with a very high mean and average, and very low std, even between the groups, is not beneficial pour any clustering model : if there are no noticeable differences between the clients, it will prove difficult to propose a comprehensive clustering mean to help Olist with customer segmentation. If the gain extracted from this new variable does not improve the model but on the contrary, flaws it, we will recommend the simple RFM approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx[\"rating_avg\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx[\"rating_avg\"].quantile([0.1, 0.25, 0.50, 0.75, 0.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 2),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "g = sns.boxplot(x=\"rating_avg\", data=df_cx, ax=ax1)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "fig.suptitle(\"Rating repartition across the dataset\")\n",
    "#\n",
    "###\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so more than 75% of the customer sample did not rate, in average, under 4. If there is something better to use as a satisfaction indicator, we'll take it. Meanwhile, let's investigate on this subsample of customers : the ones that have given a 2.5 rating or worse in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx_sample = df_cx[df_cx[\"rating_avg\"] <= 2.5]\n",
    "\n",
    "print(f\"{len(df_cx_sample)} have rated under 2.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx_sample[\"k_cluster_name\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;Ok so there's a lot of every one based from our previous clustering. Most important groups are the two majorities (which represent both arout 32/35%), which was to be expected. What is interesting is to see that our \"laggards\" from the previous segmentation (which represented 18% of the dataset) are more or less as important here as the group we called \"investors\" in the first clustering. While they both represent around 20%, we have almost an equal amount of both. Which is interesting as we did not expect the \"investor\" group to give low ratings, on the contrary. This is a good indication that we lacked informations in our RFM clustering and what we obtained during the feature engineering will likely improve the initial segmentation. In the rest of the analysis, we will also compare this group specifically and maybe diagnose the nature of this \"low\" rating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <u>2 : Delta Order / Delivery and its effects on customers' satisfaction.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>2.1 : Delta Order / Delivery : choice of variable.</u>\n",
    "\n",
    "&emsp;While we have two timedelta variables for this indicator, there is one which will likely not result in an improvement for the clustering.\n",
    "Indeed, the raw value of ∆Order/Delivery might not be as useful as expected, and ∆expected/delivery would be much more precise.\n",
    "<br> <br>\n",
    "&emsp;Let's say I preorder something (P1) on the internet that's supposed to come out in 1 month, and I order another thing (P2) that I expect in a week. If every thing goes according to what has been indicated, I will recieve P1 in a month and P2 in a week, those two orders won't have the same ∆order/delivery and that will impact negatively P1 (∆ = 1month) over P2 (∆ = 1 week).<br>\n",
    "&emsp;Now the ∆Expected/Delivered takes into account the initial expectation. So in the case all is fine and on time, ∆ will be 0 for both P1 and P2, a negative ∆ will indicate that it was delivered early and a positive one, late. This is likely way more precise.\n",
    "<br> <br>\n",
    "&emsp;We will use ∆expected/delivery from now on. Let's see it this has an impact on Cx satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = df_cx[df_cx[\"delta_days\"].notna()][\"delta_days\"].unique()\n",
    "\n",
    "x_height = dict.fromkeys(x_axis)\n",
    "\n",
    "for x in x_height:\n",
    "    x_height[x] = np.average(df_cx[(df_cx[\"delta_days\"] == x) & (df_cx[\"rating_avg\"].notna())][\"rating_avg\"].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 4),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.scatter(x=list(x_height.keys()), y=list(x_height.values()))\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Delta Days between expected and actual delivery\")\n",
    "ax1.set_ylabel(\"Average Rating for a given Delta Day\")\n",
    "fig.suptitle(\"Average rating by Days elapsed between expected and actual delivery\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :\n",
    "\n",
    "&emsp;As expected, clients' ratings are related to the punctuality of Olist. If the order arrives early or on time, clients' ratings are high, and this trend inverses as delta=0 is reached. People are more dissatisfied for every day Olist is late. The +30 days relatively high average bump in rating can probaly be attributed to the lack of orders delivered beyond 30 days. Let's check if our dataset containing our relatively dissatisfied customers contains more clients that have been delivered late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_eot_all = len(df_cx[df_cx[\"early_on_time\"] == True])\n",
    "amount_eot_sample = len(df_cx_sample[df_cx_sample[\"early_on_time\"] == True])\n",
    "\n",
    "percentage_all = (amount_eot_all / len(df_cx)) * 100\n",
    "percentage_sample = (amount_eot_sample / len(df_cx_sample)) * 100\n",
    "\n",
    "print(f\"There is {percentage_all}% of who have been delivered on time on the whole dataset\")\n",
    "print(f\"There is {percentage_sample}% of who have been delivered\\\n",
    " on time amongst the clients who have rated 2.5 or less (avg)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :\n",
    "&emsp;There is indeed a great difference between the sample of dissatisfied customers and the whole dataset.\n",
    "90.8% of the clients have received their orders early or on time on the whole dataset and just 58.2% of the clients for the sample containing people who have left a rating of 2.5 or less (avg.). We can safely say that a big part of the dissatisfied customers were disappointed because they were not delivered on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <u>3 : Delta dist, is it a determining factor in the choice made by clients ?</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>3.1 : Number of orders in function of distance cx/seller</u>\n",
    "\n",
    "### How ?\n",
    "\n",
    "&emsp;We will take the min/max value of `distance_cx_seller` and create \"distance blocks\" (array (min, max, step)) where we will count the number of orders made by clients inside the current range (`num_orders`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There is {df_cx['distance_cx_seller'].isna().sum()} NA values in the dataset\")\n",
    "print(f\"The Maximum distance is {df_cx['distance_cx_seller'].max()} Kms between seller and cx\")\n",
    "\n",
    "df_cx[\"distance_cx_seller\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;It seems we have a lot of NA (which we will fill with the median, here 434.45 and change). Let's also plot the distribution to get a better sense of the extremes, which we will include as \"more/less than whisker\" to avoid overrepresentation of the extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = df_cx[df_cx[\"distance_cx_seller\"].notna()][\"distance_cx_seller\"].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 4),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.boxplot(x=distances, vert=False, showbox=True, showmeans=True, widths=(0.4))\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Number of average kilometers between customer and seller\")\n",
    "fig.suptitle(\"Representation of clients based on their average distance to seller when order was placed\")\n",
    "ax1.set_yticks([])\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :\n",
    "\n",
    "&emsp;Looks like most people order between 0 and 2k Kms, lets zoom on this area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 4),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.boxplot(x=distances, vert=False, showbox=True, showmeans=True, widths=(0.4))\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Number of average kilometers between customer and seller\")\n",
    "fig.suptitle(\"Representation of clients based on their average distance to seller when order was placed\\\n",
    "\\nzoom on less than 2000\")\n",
    "ax1.set_xlim(left=(-100), right=(2000))\n",
    "ax1.set_xticks(range(0, 2000, 200))\n",
    "ax1.set_yticks([])\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;Let's fillna with the median, get 10/90% values and take a peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cx[\"distance_cx_seller\"] = df_cx[\"distance_cx_seller\"].fillna(df_cx[\"distance_cx_seller\"].median())\n",
    "\n",
    "distances_post_fill = df_cx[df_cx[\"distance_cx_seller\"].notna()][\"distance_cx_seller\"].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_distance = df_cx[\"distance_cx_seller\"].quantile([0.10, 0.90])\n",
    "\n",
    "llim = round(quantiles_distance[0.10], ndigits=4)  # Rounded to meter\n",
    "rlim = round(quantiles_distance[0.90], ndigits=4)  # Rounded to meter\n",
    "\n",
    "print(f\"10% above {llim} Kms, 90% under {rlim} Kms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 4),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.boxplot(x=distances_post_fill, vert=False, showbox=True, showmeans=True, widths=(0.4), sym=\"\")\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Number of average kilometers between customer and seller\")\n",
    "fig.suptitle(\"Representation of clients based on their average distance to seller when order was placed\\\n",
    "\\nzoom on less than 2000, fliers removed\")\n",
    "ax1.set_xlim(left=(-100), right=(2000))\n",
    "ax1.set_xticks(range(0, 2000, 200))\n",
    "ax1.set_yticks([])\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;Filling NA by the median didnt impact negatively our data. Let's set our range to : under 50kms, above 1500 kms with step 100kms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_range = np.arange(50, 1551, 100)\n",
    "\n",
    "distance_dict = dict.fromkeys(distance_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous = 0\n",
    "for distance in distance_range:\n",
    "    if distance != distance_range[-1]:\n",
    "        amnt_orders = df_cx[(df_cx[\"distance_cx_seller\"] > previous)\n",
    "                        & (df_cx[\"distance_cx_seller\"] < distance)][\"num_orders\"].sum()\n",
    "\n",
    "    else:  # Last distance of distance range, just need \"more than distance\"\n",
    "        print(\"last\")\n",
    "        amnt_orders = df_cx[(df_cx[\"distance_cx_seller\"] > 1550)][\"num_orders\"].sum()\n",
    "\n",
    "    distance_dict[distance] = amnt_orders        \n",
    "    previous = distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addlabels_ranged100(x, y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i * 100 + 50, y[i], f\"{y[i]}\", ha=\"center\", bbox=dict(facecolor=\"white\", alpha=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(8, 6),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "magma = plt.get_cmap(\"magma\").reversed()\n",
    "\n",
    "height_normalized = [value / max(list(distance_dict.values())) for value in list(distance_dict.values())]\n",
    "\n",
    "colors = magma(height_normalized)\n",
    "\n",
    "ax1.bar(x=list(distance_dict.keys()), height=list(distance_dict.values()), width=90, color=colors)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xticks(distance_range)\n",
    "ax1.set_xlabel(\"Average distance from seller\")\n",
    "addlabels_ranged100(x=[distance for distance in distance_dict.keys()], y=[amnt for amnt in distance_dict.values()])\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note : As we drew our limits between 10% and 90% of the dataset, the high amount of order above 1550 kms is to be expected as it represents 10% of the dataset</i>\n",
    "\n",
    "#### Observations :\n",
    "\n",
    "&emsp;This graph shows that, indeed, people orders are more often than not to seller that are relatively close, orders from seller above 550kms away begin to drop quite quickly. We can hypothesize that expected delivery time is shorter for shorter distances and clients prefer to get an item quickly. Although there is a non negligeable part of orders ranging from 650km and above, it is possible that it is for specific products that have no alternatives locally or from customers who live in more remotes part of Brazil.\n",
    "<br> <br>\n",
    "&emsp;This is a more interesting variable than expected. As we know, alternatives like Amazon for example use a subscription method like Prime to get rid of delivery fees altogether. It hints that it is not the case here or there is an incentive (fee, time etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>4 : Geolocation, are our customers more in cities etc.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample or bubles ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure()\n",
    "\n",
    "# for rfm_tuple in sample_rfm.itertuples():\n",
    "#     fig.add_trace(go.Scattergeo(\n",
    "#         # locations='brazil',\n",
    "#         # locationmode= \"country names\",\n",
    "#         lat=[rfm_tuple.lat],\n",
    "#         lon=[rfm_tuple.lon],\n",
    "#         marker = dict(\n",
    "#             line_color='rgb(40,40,40)',\n",
    "#             line_width=0.5,\n",
    "#             sizemode = 'area'\n",
    "#         ),\n",
    "#         ))\n",
    "\n",
    "# fig.update_layout(\n",
    "#         showlegend = False,\n",
    "#         width=800,\n",
    "#         height=800,\n",
    "#         geo = dict(\n",
    "#             scope = 'south america',\n",
    "#             landcolor = 'rgb(217, 217, 217)',\n",
    "#             ),\n",
    "#         margin=dict(l=20, r=20, t=20, b=20),\n",
    "#     )\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>5 : PCA on selected variables for classification</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>5.1 : Variable pre-selection and conversions</u>\n",
    "\n",
    "### Why ?\n",
    "\n",
    "&emsp;PCA will work better on certain data, while I have coded a layer of abstraction above it to make it easier to use and strealine the process, we still need to select our variables accordingly.\n",
    "\n",
    "- PCA works on numerical values, so we need a few conversions :\n",
    "    - Boolean values must go from True/False to 1/0\n",
    "    - Datetimes and timedeltas must be converted in 1. an unerstable way for the humans 2. an understandable way for the machine, so depending on the case we will use days as the main metric (most_recent_order is a repetition of recency, so we will just drop it)\n",
    "    - Cluster indicators from previous clustering will also be dropped as weel as any information regarding this (cluster name etc.)\n",
    "    - As stated, delta_delivery is lacking compared to expected/reality, which we have already converted to delta_days, so we'll drop both and keep \"delta_days\"\n",
    "    - Distance cx/seller proved to be an interesting variable so we'll keep it.\n",
    "    - For now we will also remove the lat/lon of all clients.\n",
    "    - customer_uid is purely informative and will be dropped to be re added after (to identify who's who)\n",
    "    - order_id_list is also purely informative, it will not help the model in any way\n",
    "\n",
    "<i>By default, converting a boolean column into a int column will do the 1/0 conversion for us, not necessary to reinvent the wheel there</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcols = [\n",
    "    \"customer_uid\", \"cluster_kmeans_4\", \"cluster_DBSCAN\",\n",
    "    \"k_cluster_name\", \"delta_delivery\", \"expected_reality\",\n",
    "    \"order_id_list\", \"most_recent_order_dt\", \"lat\", \"lon\"\n",
    "    ]\n",
    "\n",
    "df_model = df_cx.drop(columns=(dropcols), errors=\"ignore\")\n",
    "\n",
    "df_model.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first convert \"most_ancient_order_dt\", which we assimilate to the account creation. We want the number of days between then and now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_creation(row):\n",
    "    \"\"\"\n",
    "    Returns the difference between now and the most ancient order\n",
    "    Most ancient order is understood as the account creation date for lack\n",
    "    of more specific info\n",
    "    \"\"\"\n",
    "\n",
    "    now = np.datetime64(\"now\")\n",
    "    then = row[\"most_ancient_order_dt\"]\n",
    "    delta = now - then\n",
    "    delta = np.timedelta64(delta, \"D\")\n",
    "    return delta.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"delta_creation\"] = df_model.apply(delta_creation, axis=1)\n",
    "\n",
    "df_model = df_model.drop(columns=[\"most_ancient_order_dt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"has_rated\"] = df_model[\"has_rated\"].astype(int)\n",
    "df_model[\"has_commented\"] = df_model[\"has_commented\"].astype(int)\n",
    "df_model[\"early_on_time\"] = df_model[\"early_on_time\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>5.2 : PCA and interpretations : </u>\n",
    "\n",
    "<i>For more info about the class Easy_pca, documentation is provided in the model as docstring</i>\n",
    "\n",
    "- First we want to get a scree plot to know about the princpal components we need.\n",
    "- Then we'll want to take a look at the correlation circles to have a better grasp on what individual selected PC is.\n",
    "- For ease of use, we can also display a table with the coefficients of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epca_model = Easy_pca(dataset=df_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = epca_model.get_scree_plot(show=False)\n",
    "\n",
    "fig.set_dpi(pc_dpi)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(8)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we get around 80% cummulative inertia with PC1 through PC6. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = epca_model.display_circles(couple_pc=(0, 1), show=False)\n",
    "\n",
    "fig.set_dpi(pc_dpi)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = epca_model.display_circles(couple_pc=(2, 3), show=False)\n",
    "\n",
    "fig.set_dpi(pc_dpi)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = epca_model.display_circles(couple_pc=(4, 5), show=False)\n",
    "\n",
    "fig.set_dpi(pc_dpi)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epca_model.biplot((0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epca_model.biplot((2, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epca_model.biplot((4, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contribution = epca_model.show_contribution(lim_pc=6)\n",
    "\n",
    "df_contribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have there the coefficients of each variable, to get a better grasp on each one and to see what subset of variables contribute the most, we will use abs values for each PC, and rename it as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df_contribution[\"PC1\"]).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, PC1 represents Time. The frequency is positively correlated with PC1 while delta_creation and the recency are negatively correlated. It is basically the time since last order.\n",
    "- Proposed name : time_impact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df_contribution[\"PC2\"]).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is more of a mix in PC2, we can clearly see that it represents the customer's involvment with the order : the variables on ratings and comments are both very important in this PC. While we see delta_days scoring quite high, we already established it was highly correlated with the ratings and user's involvment.\n",
    "- Proposed name : general_involvment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df_contribution[\"PC3\"]).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PC2 and PC3 might seem similar, PC3 seems entirely focused on ratings and seems to ignore comments. early_on_time and delta_days score also high, likely for the same reason as PC2. Taking the real values (not abs), we can see that rating avg. amd comment ratio are anticorellated to the variable \"delta_days\", it is interpretable as general dissatisfaction : when the value of delta days climbs, the ratings fall, as we have established in 2.1\n",
    "\n",
    "- Proposed name : rating_delay / dissatisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df_contribution[\"PC4\"]).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute values of PC3 and PC4 are very similar, for a very good reason : while PC3 represents discontentment, PC4 represents satisfaction : delta days is highly negatively correlated with PC4, while rating average and early/on time are highly positively correlated with PC4. If delta days is low, and the order is on time or early, cx is happy.\n",
    "\n",
    "- Proposed name : overall_satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df_contribution[\"PC5\"]).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PC is highly correlated with 3 variables : monetary, num_orders and distance_cx_seller (negatively) : we established in 3.1 that order amounts decreased with the distance so it makes sense. We can think of PC5 as a variable that describes the volume of transaction and the overall sum of money people spend on Olist.\n",
    "\n",
    "- Proposed_name : value_and_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df_contribution[\"PC6\"]).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC6 is directly tied to the conclusions we drew at the end of 3.1 : distance is highly negatively correlated to PC6 while num_orders is highly positively correlated to this PC. This can represent the impact of distance on order amounts.\n",
    "\n",
    "- Proposed_name : distance_impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_keep = [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"]\n",
    "\n",
    "df_model_pca = epca_model.pcframe[pc_keep]\n",
    "\n",
    "df_model_pca.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = dict.fromkeys(pc_keep)\n",
    "\n",
    "rename_dict[\"PC1\"] = \"time_impact\"\n",
    "rename_dict[\"PC2\"] = \"general_involvment\"\n",
    "rename_dict[\"PC3\"] = \"overall_discontentment\"\n",
    "rename_dict[\"PC4\"] = \"overall_satisfaction\"\n",
    "rename_dict[\"PC5\"] = \"value_and_volume\"\n",
    "rename_dict[\"PC6\"] = \"distance_impact\"\n",
    "\n",
    "df_model_pca = df_model_pca.rename(columns=rename_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pca.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward we will use this dataset (same index) for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>6 : Conclusions, modelisations of adjusted parameters and exports</u> :\n",
    "\n",
    "- From what the exploratory analysis revealed, customer satisfaction is largely based on punctuality from Olist. The distance also plays an important role. So, before the modelisation. The early conclusion is that improvements must be done on those 2 factors :\n",
    "    - The first by improving the reliability of shipping\n",
    "    - The second by connecting more local sellers or opening wharehouses in aeras that are too remote for most sellers, as we saw that distance played a big role on the amount of orders and the repeat of orders (PCA)\n",
    "- We will, as we did with the RFM variables, try to cluster the customers based on the data we gathered post-PCA.\n",
    "    - Will be used : K-means and DBSCAN, we will not use Agglomerative clustering for the same reason we did not use it for the RFM approach, it is too resource hungry on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>6.1 : K-Means optimization</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pca.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2, 15)\n",
    "\n",
    "k_means_optimizer(data=df_model_pca, k_range=k_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Clusters seems to be the way to go, it maximizes the Silhouette Score while having an acceptable SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4)\n",
    "y_predicted = km.fit_predict(df_model_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pca[\"cluster_k4\"] = y_predicted\n",
    "df_model_pca[\"cluster_k4\"] = y_predicted  # Index was kept between the dimsensionnal reductions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 : DBSCAN Clustering :\n",
    "\n",
    "&emsp;We will also use DBSCAN with the same idea as RFM #4.2, with min points as dimension + 1 (4)\n",
    "&emsp;Epsilon can be determined using k neighbors. Using a graph to represent the avg distance between a point and its k-neighbors (here 4 : dimension + 1). Zooming in and using the elbow method help us to focus on the best potential epsilon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix = df_model_pca.drop(columns=[\"cluster_k4\"]).to_numpy()\n",
    "nneighbors = NearestNeighbors(n_neighbors=4, n_jobs=-1)  # dataset dim + 1\n",
    "\n",
    "nneighbors.fit(X=neighbors_matrix)\n",
    "\n",
    "distances, potential_eps = nneighbors.kneighbors(neighbors_matrix)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances_plot = distances[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=1,\n",
    "    figsize=(16, 8),\n",
    "    dpi=pc_dpi,\n",
    ")\n",
    "\n",
    "ax1.plot(distances_plot)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "ax1.set_xlabel(\"Object\")\n",
    "ax1.set_ylabel(\"k distance\")\n",
    "fig.suptitle(\"Points sorted by distance - Neighbors = 4\")\n",
    "#\n",
    "###\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets innovate a bit and use plotly to zoom in\n",
    "\n",
    "fig = px.line(distances_plot)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 1.7/1.9 seems to be the tipping point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = DBSCAN(eps=1.7, min_samples=6, n_jobs=-1)\n",
    "\n",
    "y_predict_dbs = dbs.fit_predict(df_model_pca.drop(columns=[\"cluster_k4\", \"cluster_DBSCAN\"], errors=\"ignore\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pca[\"cluster_DBSCAN\"] = y_predict_dbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_pca[\"cluster_DBSCAN\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Likely :\n",
    "Theres a lot more clusters than Kmeans. <br>\n",
    "Since the dataset seems linearly separable, DBSCAN doesnt do a great job at clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>6.3 : Observations of clustering both with PCA applied and using UMAP :</u>\n",
    "\n",
    "- Why not UMAP earlier ?\n",
    "While it is very good at dimensional reduction, it makes the axes hard to interpret. It helps visualizing but the more robust explanation will be through radars of the average and median of each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2214c6fb926f71468a18ca7cf946224dc61b1c77c0f95ec2e3a3e68abe6a5ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
